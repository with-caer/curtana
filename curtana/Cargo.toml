[package]
name = "curtana"
description = "Simplified zero-cost wrapper over llama.cpp powered by lama-cpp-2."
categories = [
    "api-bindings",
    "text-processing",
]
keywords = [
    "llama",
    "llm",
    "embedding",
    "inference",
]
readme = "../README.md"
version = "0.1.1"
edition.workspace = true
repository.workspace = true
license.workspace = true
authors.workspace = true

[package.metadata.release]
release = true

[package.metadata.docs.rs]
# Generate documentation with all features enabled
features = []

[features]

# Mutually-exclusive features which enable
# diferent CPU/GPU backends for the Llama APIs.
cuda = ["llama-cpp-2/cuda"]
metal = ["llama-cpp-2/metal"]
openmp = ["llama-cpp-2/openmp"]

[dependencies]
lazy_static = "1.5.0"

# OpenAI GPT prompt templating.
openai-harmony = "0.0.3"

# LLM inference.
llama-cpp-2 = { version = "0.1.121", default-features = false, features = [] }
